{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from basic_tokenizer import BasicTokenizer\n",
    "\n",
    "\n",
    "\n",
    "# from regex_tokenizer import RegexTokenizer\n",
    "\n",
    "\n",
    "\n",
    "def test_tokenizer(tok):\n",
    "\n",
    "\n",
    "    with open(\"shake.txt\", \"r\") as f:\n",
    "\n",
    "\n",
    "        shake = f.read()\n",
    "\n",
    "\n",
    "    starting_shake = shake[:100_000]\n",
    "\n",
    "\n",
    "    test_set = shake[100_000:150_000]\n",
    "\n",
    "    print(len(shake))\n",
    "\n",
    "\n",
    "    tok.train(starting_shake, 300, True)\n",
    "\n",
    "\n",
    "    encoded = tok.encode(test_set)\n",
    "\n",
    "\n",
    "    decoded = tok.decode(encoded)\n",
    "\n",
    "    assert test_set == decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RegexTokenizer' object has no attribute 'pattern'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_tokenizer(\u001b[43mRegexTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\JeanLELONG\\inno\\scratch_gpt\\regex_tokenizer.py:10\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      8\u001b[0m super().__init__()\n\u001b[0;32m      9\u001b[0m self.GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n\u001b[1;32m---> 10\u001b[0m self.compiled_pattern = re.compile(self.pattern)\n\u001b[0;32m     11\u001b[0m \n\u001b[0;32m     12\u001b[0m self.merges = {}  # key: pair of bytes\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RegexTokenizer' object has no attribute 'pattern'"
     ]
    }
   ],
   "source": [
    "test_tokenizer(RegexTokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115393\n",
      "replacing (101, 32) by 256 occuring 2734 times \n",
      "replacing (116, 104) by 257 occuring 2054 times \n",
      "replacing (116, 32) by 258 occuring 1441 times \n",
      "replacing (115, 32) by 259 occuring 1383 times \n",
      "replacing (111, 117) by 260 occuring 1327 times \n",
      "replacing (44, 32) by 261 occuring 1160 times \n",
      "replacing (100, 32) by 262 occuring 1133 times \n",
      "replacing (101, 114) by 263 occuring 997 times \n",
      "replacing (97, 110) by 264 occuring 858 times \n",
      "replacing (105, 110) by 265 occuring 834 times \n",
      "replacing (58, 10) by 266 occuring 820 times \n",
      "replacing (32, 257) by 267 occuring 811 times \n",
      "replacing (101, 110) by 268 occuring 774 times \n",
      "replacing (10, 10) by 269 occuring 735 times \n",
      "replacing (111, 114) by 270 occuring 703 times \n",
      "replacing (111, 110) by 271 occuring 674 times \n",
      "replacing (121, 32) by 272 occuring 659 times \n",
      "replacing (97, 114) by 273 occuring 637 times \n",
      "replacing (111, 32) by 274 occuring 603 times \n",
      "replacing (104, 97) by 275 occuring 575 times \n",
      "replacing (108, 108) by 276 occuring 569 times \n",
      "replacing (121, 260) by 277 occuring 548 times \n",
      "replacing (46, 269) by 278 occuring 519 times \n",
      "replacing (85, 83) by 279 occuring 497 times \n",
      "replacing (279, 266) by 280 occuring 494 times \n",
      "replacing (101, 115) by 281 occuring 489 times \n",
      "replacing (44, 10) by 282 occuring 462 times \n",
      "replacing (104, 105) by 283 occuring 457 times \n",
      "replacing (114, 32) by 284 occuring 429 times \n",
      "replacing (267, 256) by 285 occuring 393 times \n",
      "replacing (110, 111) by 286 occuring 363 times \n",
      "replacing (116, 105) by 287 occuring 355 times \n",
      "replacing (111, 109) by 288 occuring 344 times \n",
      "replacing (276, 32) by 289 occuring 317 times \n",
      "replacing (73, 280) by 290 occuring 315 times \n",
      "replacing (111, 102) by 291 occuring 307 times \n",
      "replacing (101, 97) by 292 occuring 306 times \n",
      "replacing (264, 262) by 293 occuring 302 times \n",
      "replacing (116, 274) by 294 occuring 298 times \n",
      "replacing (73, 32) by 295 occuring 296 times \n",
      "replacing (117, 115) by 296 occuring 286 times \n",
      "replacing (119, 105) by 297 occuring 270 times \n",
      "replacing (104, 256) by 298 occuring 265 times \n",
      "replacing (111, 119) by 299 occuring 257 times \n",
      "------------------------------\n",
      "100_000 tokens\n",
      "69_965 ids\n",
      "1.43x compression ratio\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_tokenizer(BasicTokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
